{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ERiQlIzniySo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import BertPreTokenizer\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gutenberg_text(raw_text):\n",
        "    \"\"\"Cleans Project Gutenberg headers/footers.\"\"\"\n",
        "    # Use non-greedy '.*?' to match the shortest possible text for the title\n",
        "    start_marker = r\"\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .*? \\*\\*\\*\"\n",
        "    end_marker = r\"\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .*? \\*\\*\\*\"\n",
        "\n",
        "    start_match = re.search(start_marker, raw_text, re.IGNORECASE | re.DOTALL)\n",
        "    end_match = re.search(end_marker, raw_text, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    if not start_match:\n",
        "        print(\"  [Warning] Could not find START marker.\")\n",
        "        if end_match:\n",
        "            return raw_text[:end_match.start()].strip()\n",
        "        return raw_text.strip()\n",
        "\n",
        "    if not end_match:\n",
        "        print(\"  [Warning] Could not find END marker.\")\n",
        "        return raw_text[start_match.end():].strip()\n",
        "\n",
        "    text_start = start_match.end()\n",
        "    text_end = end_match.start()\n",
        "\n",
        "    cleaned_text = raw_text[text_start:text_end].strip()\n",
        "    return cleaned_text\n",
        "\n",
        "BOOKS_TO_DOWNLOAD = {\n",
        "    \"Frankenstein\": \"https://www.gutenberg.org/ebooks/84.txt.utf-8\",\n",
        "    \"The_Time_Machine\": \"https://www.gutenberg.org/ebooks/35.txt.utf-8\",\n",
        "    \"The_War_of_the_Worlds\": \"https://www.gutenberg.org/ebooks/36.txt.utf-8\",\n",
        "    \"20000_Leagues_Under_the_Sea\": \"https://www.gutenberg.org/ebooks/164.txt.utf-8\",\n",
        "    \"A_Princess_of_Mars\": \"https://www.gutenberg.org/ebooks/62.txt.utf-8\"\n",
        "}\n",
        "\n",
        "OUTPUT_FILENAME = \"scifi_corpus.txt\"\n",
        "all_cleaned_texts = []\n",
        "\n",
        "print(\"Starting download process...\")\n",
        "\n",
        "for title, url in BOOKS_TO_DOWNLOAD.items():\n",
        "    print(f\"Fetching '{title}'...\", end=\"\")\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        response.encoding = 'utf-8'\n",
        "        raw_text = response.text\n",
        "\n",
        "        cleaned_text = clean_gutenberg_text(raw_text)\n",
        "        all_cleaned_texts.append(cleaned_text)\n",
        "        print(\" Done.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\n  [ERROR] Failed to download {title}: {e}\")\n",
        "\n",
        "if all_cleaned_texts:\n",
        "    print(f\"\\nCombining {len(all_cleaned_texts)} books into '{OUTPUT_FILENAME}'...\")\n",
        "\n",
        "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
        "        full_corpus = \"\\n\\n\\n\".join(all_cleaned_texts)\n",
        "        f.write(full_corpus)\n",
        "\n",
        "    print(f\"Successfully created '{OUTPUT_FILENAME}' with a total of {len(full_corpus)} characters.\")\n",
        "else:\n",
        "    print(\"No books were downloaded. Output file was not created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ZMCsMjlhUg",
        "outputId": "ec094cde-80a5-4150-e854-16e5a337e3be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download process...\n",
            "Fetching 'Frankenstein'... Done.\n",
            "Fetching 'The_Time_Machine'... Done.\n",
            "Fetching 'The_War_of_the_Worlds'... Done.\n",
            "Fetching '20000_Leagues_Under_the_Sea'... Done.\n",
            "Fetching 'A_Princess_of_Mars'... Done.\n",
            "\n",
            "Combining 5 books into 'scifi_corpus.txt'...\n",
            "Successfully created 'scifi_corpus.txt' with a total of 1941792 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d=256, p=32, num_heads=2, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.p = p\n",
        "    self.d = d\n",
        "    self.w_k = nn.ModuleList([nn.Linear(d, p, bias = False) for _ in range(num_heads)])\n",
        "    self.w_q = nn.ModuleList([nn.Linear(d, p, bias = False) for _ in range(num_heads)])\n",
        "    self.w_v = nn.ModuleList([nn.Linear(d, p, bias = False) for _ in range(num_heads)])\n",
        "\n",
        "    self.w_o = nn.Linear(num_heads * p, d, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale = self.p ** -0.5\n",
        "\n",
        "  def forward(self, h):\n",
        "    _, t, d = h.shape\n",
        "    head_outputs = []\n",
        "\n",
        "    mask = torch.triu(torch.ones(t, t), diagonal=1).bool().to(h.device)\n",
        "\n",
        "    for m in range(self.num_heads):\n",
        "        k = self.w_k[m](h)\n",
        "        q = self.w_q[m](h)\n",
        "        v = self.w_v[m](h)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        head_out = torch.matmul(attn_weights, v)\n",
        "        head_outputs.append(head_out)\n",
        "\n",
        "    h_prime = torch.cat(head_outputs, dim=-1)\n",
        "    output = self.w_o(h_prime)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "I552aM4UlmU6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "  def __init__(self, d=256):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(d, 2*d, bias=False)\n",
        "    self.fc2 = nn.Linear(2*d, d, bias=False)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, h):\n",
        "    return self.fc2(self.relu(self.fc1(h)))"
      ],
      "metadata": {
        "id": "zZs0MzuG5SEF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d=256, p=32, num_heads=2, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(d, p, num_heads, dropout)\n",
        "    self.norm1 = nn.LayerNorm(d)\n",
        "    self.mlp = PositionWiseFFN(d)\n",
        "    self.norm2 = nn.LayerNorm(d)\n",
        "\n",
        "  def forward(self, h):\n",
        "    h = self.norm1(h + self.attention(h))\n",
        "    h = self.norm2(h + self.mlp(h))\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "dn4G7pqR8SzL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d=256, max_len=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d, 2).float() * (-np.log(10000.0) / d))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T, :].unsqueeze(0)"
      ],
      "metadata": {
        "id": "PRMXOj1fBTug"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d=256, p=32, num_heads=2, num_blocks=3,\n",
        "                 max_len=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d = d\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d)\n",
        "        self.pos_encoding = PositionalEncoding(d, max_len)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d, p, num_heads, dropout)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.output = nn.Linear(d, vocab_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.token_embedding(x)\n",
        "        h = self.pos_encoding(h)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "\n",
        "        logits = self.output(h)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "6AS-v9GGA58L"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59ef291f"
      },
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokens, context_length=128):\n",
        "        self.tokens = tokens\n",
        "        self.context_length = context_length\n",
        "\n",
        "        self.sequences = []\n",
        "        for i in range(0, len(tokens) - context_length, context_length // 2):\n",
        "            seq = tokens[i:i + context_length + 1]\n",
        "            if len(seq) == context_length + 1:\n",
        "                self.sequences.append(seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        x = torch.tensor(seq[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(seq[1:], dtype=torch.long)\n",
        "        return x, y"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_tokenize_data(data_path, vocab_size=2048):\n",
        "    with open('scifi_corpus.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
        "    tokenizer.pre_tokenizer = BertPreTokenizer()\n",
        "\n",
        "    trainer = BpeTrainer(special_tokens=['[UNK]', '[CLS]', '[PAD]', '[SEP]', '[MASK]'])\n",
        "\n",
        "    tokenizer.train_from_iterator([text], trainer)\n",
        "\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "\n",
        "    print(f\"Corpus length: {len(text)} characters\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "    print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "    return tokens, tokenizer"
      ],
      "metadata": {
        "id": "UVC_45fHFMXO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-4):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    iterations = []\n",
        "    iteration = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "\n",
        "      for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y, = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "\n",
        "        B, T, V = logits.shape\n",
        "        logits = logits.view(B * T, V)\n",
        "        y = y.view(B * T)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "        iterations.append(iteration)\n",
        "        iteration += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Validation every 1000 iterations\n",
        "        if iteration % 1000 == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for x_val, y_val in val_loader:\n",
        "                    x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "                    logits_val = model(x_val)\n",
        "                    B, T, V = logits_val.shape\n",
        "                    logits_val = logits_val.view(B * T, V)\n",
        "                    y_val = y_val.view(B * T)\n",
        "                    val_loss += criterion(logits_val, y_val).item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_losses.append(val_loss)\n",
        "            print(f\"Validation Loss at iteration {iteration}: {val_loss:.4f}\")\n",
        "            model.train()\n",
        "\n",
        "      avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "      print(f\"Epoch {epoch+1} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, iterations"
      ],
      "metadata": {
        "id": "XaH9SicEBzOC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses, iterations, save_path='training_loss.pdf'):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    window = 50\n",
        "    if len(train_losses) > window:\n",
        "        train_smooth = np.convolve(train_losses, np.ones(window)/window, mode='valid')\n",
        "        iter_smooth = iterations[window-1:]\n",
        "    else:\n",
        "        train_smooth = train_losses\n",
        "        iter_smooth = iterations\n",
        "\n",
        "    plt.plot(iter_smooth, train_smooth, label='Training Loss (smoothed)', alpha=0.8)\n",
        "\n",
        "    val_iterations = [i for i in iterations if i % 1000 == 0][:len(val_losses)]\n",
        "    if val_losses:\n",
        "        plt.scatter(val_iterations, val_losses, color='red', label='Validation Loss',\n",
        "                   s=50, zorder=5)\n",
        "\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {save_path}\")"
      ],
      "metadata": {
        "id": "5zmnl-rgE_Er"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 2048\n",
        "CONTEXT_LENGTH = 128\n",
        "D_MODEL = 256\n",
        "P = 32\n",
        "NUM_HEADS = 2\n",
        "NUM_BLOCKS = 5\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10000\n",
        "LR = 3e-4\n",
        "DROPOUT = 0.1"
      ],
      "metadata": {
        "id": "sXp-tsnYF1-k"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and tokenize data\n",
        "print(\"Loading and tokenizing data...\")\n",
        "tokens, tokenizer = load_and_tokenize_data('scifi_corpus.txt', vocab_size=VOCAB_SIZE)\n",
        "\n",
        "# Split into train and validation\n",
        "split_idx = int(0.9 * len(tokens))\n",
        "train_tokens = tokens[:split_idx]\n",
        "val_tokens = tokens[split_idx:]\n",
        "\n",
        "print(f\"Train tokens: {len(train_tokens)}\")\n",
        "print(f\"Validation tokens: {len(val_tokens)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TextDataset(train_tokens, CONTEXT_LENGTH)\n",
        "val_dataset = TextDataset(val_tokens, CONTEXT_LENGTH)\n",
        "\n",
        "print(f\"Train sequences: {len(train_dataset)}\")\n",
        "print(f\"Validation sequences: {len(val_dataset)}\")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "# Create model\n",
        "print(\"\\nCreating model...\")\n",
        "model = TransformerLM(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    d=D_MODEL,\n",
        "    p=P,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_blocks=NUM_BLOCKS,\n",
        "    max_len=CONTEXT_LENGTH,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Train model\n",
        "print(\"\\nTraining model...\")\n",
        "train_losses, val_losses, iterations = train_model(\n",
        "    model, train_loader, val_loader, device, epochs=EPOCHS, lr=LR\n",
        ")\n",
        "\n",
        "# Plot losses\n",
        "plot_losses(train_losses, val_losses, iterations)\n",
        "\n",
        "# Save model and tokenizer\n",
        "torch.save(model.state_dict(), 'transformer_model.pt')\n",
        "tokenizer.save('tokenizer.json')\n",
        "print(\"\\nModel and tokenizer saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Zg5CkWFFM2",
        "outputId": "e2706d9c-e221-49d2-9779-def4c0cf2c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and tokenizing data...\n",
            "Corpus length: 1905366 characters\n",
            "Number of tokens: 399830\n",
            "Vocabulary size: 25902\n",
            "Train tokens: 359847\n",
            "Validation tokens: 39983\n",
            "Train sequences: 5621\n",
            "Validation sequences: 623\n",
            "\n",
            "Creating model...\n",
            "Total parameters: 14,905,344\n",
            "\n",
            "Training model...\n",
            "Epoch 1/10000, Batch 0/176, Loss: 10.3389\n",
            "Epoch 1/10000, Batch 100/176, Loss: 6.3054\n",
            "Epoch 1 completed. Average Loss: 6.6589\n",
            "Epoch 2/10000, Batch 0/176, Loss: 5.9248\n",
            "Epoch 2/10000, Batch 100/176, Loss: 5.5534\n",
            "Epoch 2 completed. Average Loss: 5.6396\n",
            "Epoch 3/10000, Batch 0/176, Loss: 5.4801\n",
            "Epoch 3/10000, Batch 100/176, Loss: 5.3754\n",
            "Epoch 3 completed. Average Loss: 5.3319\n",
            "Epoch 4/10000, Batch 0/176, Loss: 5.1781\n",
            "Epoch 4/10000, Batch 100/176, Loss: 5.1402\n",
            "Epoch 4 completed. Average Loss: 5.1121\n",
            "Epoch 5/10000, Batch 0/176, Loss: 4.9765\n",
            "Epoch 5/10000, Batch 100/176, Loss: 4.9201\n",
            "Epoch 5 completed. Average Loss: 4.9324\n",
            "Epoch 6/10000, Batch 0/176, Loss: 4.8449\n",
            "Epoch 6/10000, Batch 100/176, Loss: 4.7098\n",
            "Validation Loss at iteration 1000: 5.6012\n",
            "Epoch 6 completed. Average Loss: 4.7751\n",
            "Epoch 7/10000, Batch 0/176, Loss: 4.7913\n",
            "Epoch 7/10000, Batch 100/176, Loss: 4.6482\n",
            "Epoch 7 completed. Average Loss: 4.6315\n",
            "Epoch 8/10000, Batch 0/176, Loss: 4.5604\n",
            "Epoch 8/10000, Batch 100/176, Loss: 4.4596\n",
            "Epoch 8 completed. Average Loss: 4.4961\n",
            "Epoch 9/10000, Batch 0/176, Loss: 4.3971\n",
            "Epoch 9/10000, Batch 100/176, Loss: 4.3507\n",
            "Epoch 9 completed. Average Loss: 4.3679\n",
            "Epoch 10/10000, Batch 0/176, Loss: 4.2559\n",
            "Epoch 10/10000, Batch 100/176, Loss: 4.2452\n",
            "Epoch 10 completed. Average Loss: 4.2439\n",
            "Epoch 11/10000, Batch 0/176, Loss: 4.1410\n",
            "Epoch 11/10000, Batch 100/176, Loss: 4.0514\n",
            "Epoch 11 completed. Average Loss: 4.1195\n",
            "Epoch 12/10000, Batch 0/176, Loss: 3.9852\n",
            "Validation Loss at iteration 2000: 5.6900\n",
            "Epoch 12/10000, Batch 100/176, Loss: 4.0522\n",
            "Epoch 12 completed. Average Loss: 4.0031\n",
            "Epoch 13/10000, Batch 0/176, Loss: 3.8987\n",
            "Epoch 13/10000, Batch 100/176, Loss: 3.8727\n",
            "Epoch 13 completed. Average Loss: 3.8859\n",
            "Epoch 14/10000, Batch 0/176, Loss: 3.7403\n",
            "Epoch 14/10000, Batch 100/176, Loss: 3.6793\n",
            "Epoch 14 completed. Average Loss: 3.7725\n",
            "Epoch 15/10000, Batch 0/176, Loss: 3.5670\n",
            "Epoch 15/10000, Batch 100/176, Loss: 3.6549\n",
            "Epoch 15 completed. Average Loss: 3.6656\n",
            "Epoch 16/10000, Batch 0/176, Loss: 3.5423\n",
            "Epoch 16/10000, Batch 100/176, Loss: 3.5548\n",
            "Epoch 16 completed. Average Loss: 3.5570\n",
            "Epoch 17/10000, Batch 0/176, Loss: 3.4033\n",
            "Epoch 17/10000, Batch 100/176, Loss: 3.3890\n",
            "Epoch 17 completed. Average Loss: 3.4534\n",
            "Epoch 18/10000, Batch 0/176, Loss: 3.2878\n",
            "Validation Loss at iteration 3000: 5.9349\n",
            "Epoch 18/10000, Batch 100/176, Loss: 3.3394\n",
            "Epoch 18 completed. Average Loss: 3.3546\n",
            "Epoch 19/10000, Batch 0/176, Loss: 3.2558\n",
            "Epoch 19/10000, Batch 100/176, Loss: 3.2562\n",
            "Epoch 19 completed. Average Loss: 3.2562\n",
            "Epoch 20/10000, Batch 0/176, Loss: 3.1546\n",
            "Epoch 20/10000, Batch 100/176, Loss: 3.2329\n",
            "Epoch 20 completed. Average Loss: 3.1648\n",
            "Epoch 21/10000, Batch 0/176, Loss: 3.0316\n",
            "Epoch 21/10000, Batch 100/176, Loss: 3.0320\n",
            "Epoch 21 completed. Average Loss: 3.0777\n",
            "Epoch 22/10000, Batch 0/176, Loss: 2.9670\n",
            "Epoch 22/10000, Batch 100/176, Loss: 2.9728\n",
            "Epoch 22 completed. Average Loss: 2.9934\n",
            "Epoch 23/10000, Batch 0/176, Loss: 2.9040\n",
            "Epoch 23/10000, Batch 100/176, Loss: 2.8616\n",
            "Validation Loss at iteration 4000: 6.2899\n",
            "Epoch 23 completed. Average Loss: 2.9125\n",
            "Epoch 24/10000, Batch 0/176, Loss: 2.8321\n",
            "Epoch 24/10000, Batch 100/176, Loss: 2.9283\n",
            "Epoch 24 completed. Average Loss: 2.8347\n",
            "Epoch 25/10000, Batch 0/176, Loss: 2.6225\n",
            "Epoch 25/10000, Batch 100/176, Loss: 2.7667\n",
            "Epoch 25 completed. Average Loss: 2.7633\n",
            "Epoch 26/10000, Batch 0/176, Loss: 2.5955\n",
            "Epoch 26/10000, Batch 100/176, Loss: 2.7176\n",
            "Epoch 26 completed. Average Loss: 2.6927\n",
            "Epoch 27/10000, Batch 0/176, Loss: 2.5091\n",
            "Epoch 27/10000, Batch 100/176, Loss: 2.6434\n",
            "Epoch 27 completed. Average Loss: 2.6249\n",
            "Epoch 28/10000, Batch 0/176, Loss: 2.4477\n",
            "Epoch 28/10000, Batch 100/176, Loss: 2.5896\n",
            "Epoch 28 completed. Average Loss: 2.5627\n",
            "Epoch 29/10000, Batch 0/176, Loss: 2.4975\n",
            "Validation Loss at iteration 5000: 6.7079\n",
            "Epoch 29/10000, Batch 100/176, Loss: 2.5115\n",
            "Epoch 29 completed. Average Loss: 2.4973\n",
            "Epoch 30/10000, Batch 0/176, Loss: 2.3401\n",
            "Epoch 30/10000, Batch 100/176, Loss: 2.3965\n",
            "Epoch 30 completed. Average Loss: 2.4389\n",
            "Epoch 31/10000, Batch 0/176, Loss: 2.3548\n",
            "Epoch 31/10000, Batch 100/176, Loss: 2.4529\n",
            "Epoch 31 completed. Average Loss: 2.3835\n",
            "Epoch 32/10000, Batch 0/176, Loss: 2.2695\n",
            "Epoch 32/10000, Batch 100/176, Loss: 2.3658\n",
            "Epoch 32 completed. Average Loss: 2.3272\n",
            "Epoch 33/10000, Batch 0/176, Loss: 2.2165\n",
            "Epoch 33/10000, Batch 100/176, Loss: 2.3429\n",
            "Epoch 33 completed. Average Loss: 2.2751\n",
            "Epoch 34/10000, Batch 0/176, Loss: 2.1659\n",
            "Epoch 34/10000, Batch 100/176, Loss: 2.2431\n",
            "Epoch 34 completed. Average Loss: 2.2255\n",
            "Epoch 35/10000, Batch 0/176, Loss: 2.0999\n",
            "Validation Loss at iteration 6000: 7.1332\n",
            "Epoch 35/10000, Batch 100/176, Loss: 2.1667\n",
            "Epoch 35 completed. Average Loss: 2.1777\n",
            "Epoch 36/10000, Batch 0/176, Loss: 2.0869\n",
            "Epoch 36/10000, Batch 100/176, Loss: 2.1153\n",
            "Epoch 36 completed. Average Loss: 2.1296\n",
            "Epoch 37/10000, Batch 0/176, Loss: 2.0422\n",
            "Epoch 37/10000, Batch 100/176, Loss: 2.0868\n",
            "Epoch 37 completed. Average Loss: 2.0870\n",
            "Epoch 38/10000, Batch 0/176, Loss: 1.9683\n",
            "Epoch 38/10000, Batch 100/176, Loss: 2.0449\n",
            "Epoch 38 completed. Average Loss: 2.0426\n",
            "Epoch 39/10000, Batch 0/176, Loss: 1.8870\n",
            "Epoch 39/10000, Batch 100/176, Loss: 2.0046\n",
            "Epoch 39 completed. Average Loss: 2.0005\n",
            "Epoch 40/10000, Batch 0/176, Loss: 1.9026\n",
            "Epoch 40/10000, Batch 100/176, Loss: 1.9411\n",
            "Validation Loss at iteration 7000: 7.5529\n",
            "Epoch 40 completed. Average Loss: 1.9587\n",
            "Epoch 41/10000, Batch 0/176, Loss: 1.8514\n",
            "Epoch 41/10000, Batch 100/176, Loss: 1.8605\n",
            "Epoch 41 completed. Average Loss: 1.9202\n",
            "Epoch 42/10000, Batch 0/176, Loss: 1.8552\n",
            "Epoch 42/10000, Batch 100/176, Loss: 1.9217\n",
            "Epoch 42 completed. Average Loss: 1.8858\n",
            "Epoch 43/10000, Batch 0/176, Loss: 1.8397\n",
            "Epoch 43/10000, Batch 100/176, Loss: 1.8679\n",
            "Epoch 43 completed. Average Loss: 1.8500\n",
            "Epoch 44/10000, Batch 0/176, Loss: 1.7621\n",
            "Epoch 44/10000, Batch 100/176, Loss: 1.8114\n",
            "Epoch 44 completed. Average Loss: 1.8122\n",
            "Epoch 45/10000, Batch 0/176, Loss: 1.7484\n",
            "Epoch 45/10000, Batch 100/176, Loss: 1.8135\n",
            "Epoch 45 completed. Average Loss: 1.7800\n",
            "Epoch 46/10000, Batch 0/176, Loss: 1.7203\n",
            "Validation Loss at iteration 8000: 7.9776\n",
            "Epoch 46/10000, Batch 100/176, Loss: 1.7831\n",
            "Epoch 46 completed. Average Loss: 1.7467\n",
            "Epoch 47/10000, Batch 0/176, Loss: 1.6113\n",
            "Epoch 47/10000, Batch 100/176, Loss: 1.7206\n",
            "Epoch 47 completed. Average Loss: 1.7166\n",
            "Epoch 48/10000, Batch 0/176, Loss: 1.6136\n",
            "Epoch 48/10000, Batch 100/176, Loss: 1.7410\n",
            "Epoch 48 completed. Average Loss: 1.6845\n",
            "Epoch 49/10000, Batch 0/176, Loss: 1.5705\n",
            "Epoch 49/10000, Batch 100/176, Loss: 1.6704\n",
            "Epoch 49 completed. Average Loss: 1.6566\n",
            "Epoch 50/10000, Batch 0/176, Loss: 1.5427\n",
            "Epoch 50/10000, Batch 100/176, Loss: 1.6251\n",
            "Epoch 50 completed. Average Loss: 1.6266\n",
            "Epoch 51/10000, Batch 0/176, Loss: 1.5653\n",
            "Epoch 51/10000, Batch 100/176, Loss: 1.5771\n",
            "Epoch 51 completed. Average Loss: 1.5986\n",
            "Epoch 52/10000, Batch 0/176, Loss: 1.4906\n",
            "Validation Loss at iteration 9000: 8.3702\n",
            "Epoch 52/10000, Batch 100/176, Loss: 1.5626\n",
            "Epoch 52 completed. Average Loss: 1.5728\n",
            "Epoch 53/10000, Batch 0/176, Loss: 1.4877\n",
            "Epoch 53/10000, Batch 100/176, Loss: 1.5788\n",
            "Epoch 53 completed. Average Loss: 1.5454\n",
            "Epoch 54/10000, Batch 0/176, Loss: 1.4521\n",
            "Epoch 54/10000, Batch 100/176, Loss: 1.5323\n",
            "Epoch 54 completed. Average Loss: 1.5237\n",
            "Epoch 55/10000, Batch 0/176, Loss: 1.5100\n",
            "Epoch 55/10000, Batch 100/176, Loss: 1.5238\n",
            "Epoch 55 completed. Average Loss: 1.4966\n",
            "Epoch 56/10000, Batch 0/176, Loss: 1.3871\n",
            "Epoch 56/10000, Batch 100/176, Loss: 1.4359\n",
            "Epoch 56 completed. Average Loss: 1.4727\n",
            "Epoch 57/10000, Batch 0/176, Loss: 1.4160\n",
            "Epoch 57/10000, Batch 100/176, Loss: 1.4838\n",
            "Validation Loss at iteration 10000: 8.7677\n",
            "Epoch 57 completed. Average Loss: 1.4458\n",
            "Epoch 58/10000, Batch 0/176, Loss: 1.3965\n",
            "Epoch 58/10000, Batch 100/176, Loss: 1.3772\n",
            "Epoch 58 completed. Average Loss: 1.4278\n",
            "Epoch 59/10000, Batch 0/176, Loss: 1.3585\n",
            "Epoch 59/10000, Batch 100/176, Loss: 1.3863\n",
            "Epoch 59 completed. Average Loss: 1.4069\n",
            "Epoch 60/10000, Batch 0/176, Loss: 1.3189\n",
            "Epoch 60/10000, Batch 100/176, Loss: 1.4045\n",
            "Epoch 60 completed. Average Loss: 1.3829\n",
            "Epoch 61/10000, Batch 0/176, Loss: 1.2735\n",
            "Epoch 61/10000, Batch 100/176, Loss: 1.3099\n",
            "Epoch 61 completed. Average Loss: 1.3616\n",
            "Epoch 62/10000, Batch 0/176, Loss: 1.3252\n",
            "Epoch 62/10000, Batch 100/176, Loss: 1.3684\n",
            "Epoch 62 completed. Average Loss: 1.3448\n",
            "Epoch 63/10000, Batch 0/176, Loss: 1.2681\n",
            "Validation Loss at iteration 11000: 9.0980\n",
            "Epoch 63/10000, Batch 100/176, Loss: 1.3706\n",
            "Epoch 63 completed. Average Loss: 1.3254\n",
            "Epoch 64/10000, Batch 0/176, Loss: 1.2649\n",
            "Epoch 64/10000, Batch 100/176, Loss: 1.3402\n",
            "Epoch 64 completed. Average Loss: 1.3065\n",
            "Epoch 65/10000, Batch 0/176, Loss: 1.1963\n",
            "Epoch 65/10000, Batch 100/176, Loss: 1.2380\n",
            "Epoch 65 completed. Average Loss: 1.2855\n",
            "Epoch 66/10000, Batch 0/176, Loss: 1.2099\n",
            "Epoch 66/10000, Batch 100/176, Loss: 1.2625\n",
            "Epoch 66 completed. Average Loss: 1.2695\n",
            "Epoch 67/10000, Batch 0/176, Loss: 1.1904\n",
            "Epoch 67/10000, Batch 100/176, Loss: 1.2599\n",
            "Epoch 67 completed. Average Loss: 1.2510\n",
            "Epoch 68/10000, Batch 0/176, Loss: 1.1776\n",
            "Epoch 68/10000, Batch 100/176, Loss: 1.1841\n",
            "Epoch 68 completed. Average Loss: 1.2377\n",
            "Epoch 69/10000, Batch 0/176, Loss: 1.1630\n",
            "Validation Loss at iteration 12000: 9.4353\n",
            "Epoch 69/10000, Batch 100/176, Loss: 1.1954\n",
            "Epoch 69 completed. Average Loss: 1.2154\n",
            "Epoch 70/10000, Batch 0/176, Loss: 1.1520\n",
            "Epoch 70/10000, Batch 100/176, Loss: 1.1980\n",
            "Epoch 70 completed. Average Loss: 1.2004\n",
            "Epoch 71/10000, Batch 0/176, Loss: 1.0878\n",
            "Epoch 71/10000, Batch 100/176, Loss: 1.1731\n",
            "Epoch 71 completed. Average Loss: 1.1835\n",
            "Epoch 72/10000, Batch 0/176, Loss: 1.1458\n",
            "Epoch 72/10000, Batch 100/176, Loss: 1.2095\n",
            "Epoch 72 completed. Average Loss: 1.1640\n",
            "Epoch 73/10000, Batch 0/176, Loss: 1.0821\n",
            "Epoch 73/10000, Batch 100/176, Loss: 1.1850\n",
            "Epoch 73 completed. Average Loss: 1.1532\n",
            "Epoch 74/10000, Batch 0/176, Loss: 1.0856\n",
            "Epoch 74/10000, Batch 100/176, Loss: 1.1295\n",
            "Validation Loss at iteration 13000: 9.7885\n",
            "Epoch 74 completed. Average Loss: 1.1391\n",
            "Epoch 75/10000, Batch 0/176, Loss: 1.0886\n",
            "Epoch 75/10000, Batch 100/176, Loss: 1.1440\n",
            "Epoch 75 completed. Average Loss: 1.1233\n",
            "Epoch 76/10000, Batch 0/176, Loss: 1.0809\n",
            "Epoch 76/10000, Batch 100/176, Loss: 1.1560\n",
            "Epoch 76 completed. Average Loss: 1.1090\n",
            "Epoch 77/10000, Batch 0/176, Loss: 1.0606\n",
            "Epoch 77/10000, Batch 100/176, Loss: 1.0812\n",
            "Epoch 77 completed. Average Loss: 1.0967\n",
            "Epoch 78/10000, Batch 0/176, Loss: 1.0493\n",
            "Epoch 78/10000, Batch 100/176, Loss: 1.1241\n",
            "Epoch 78 completed. Average Loss: 1.0837\n",
            "Epoch 79/10000, Batch 0/176, Loss: 1.0033\n",
            "Epoch 79/10000, Batch 100/176, Loss: 1.0930\n",
            "Epoch 79 completed. Average Loss: 1.0716\n",
            "Epoch 80/10000, Batch 0/176, Loss: 1.0271\n",
            "Validation Loss at iteration 14000: 10.0874\n",
            "Epoch 80/10000, Batch 100/176, Loss: 1.0709\n",
            "Epoch 80 completed. Average Loss: 1.0570\n",
            "Epoch 81/10000, Batch 0/176, Loss: 0.9820\n",
            "Epoch 81/10000, Batch 100/176, Loss: 1.0777\n",
            "Epoch 81 completed. Average Loss: 1.0441\n",
            "Epoch 82/10000, Batch 0/176, Loss: 0.9915\n",
            "Epoch 82/10000, Batch 100/176, Loss: 1.0677\n",
            "Epoch 82 completed. Average Loss: 1.0291\n",
            "Epoch 83/10000, Batch 0/176, Loss: 1.0083\n",
            "Epoch 83/10000, Batch 100/176, Loss: 1.0363\n",
            "Epoch 83 completed. Average Loss: 1.0215\n",
            "Epoch 84/10000, Batch 0/176, Loss: 0.9250\n",
            "Epoch 84/10000, Batch 100/176, Loss: 0.9905\n",
            "Epoch 84 completed. Average Loss: 1.0077\n",
            "Epoch 85/10000, Batch 0/176, Loss: 0.9215\n",
            "Epoch 85/10000, Batch 100/176, Loss: 1.0367\n",
            "Epoch 85 completed. Average Loss: 0.9994\n",
            "Epoch 86/10000, Batch 0/176, Loss: 0.9445\n",
            "Validation Loss at iteration 15000: 10.4088\n",
            "Epoch 86/10000, Batch 100/176, Loss: 0.9810\n",
            "Epoch 86 completed. Average Loss: 0.9842\n",
            "Epoch 87/10000, Batch 0/176, Loss: 0.9576\n",
            "Epoch 87/10000, Batch 100/176, Loss: 0.9849\n",
            "Epoch 87 completed. Average Loss: 0.9734\n",
            "Epoch 88/10000, Batch 0/176, Loss: 0.9022\n",
            "Epoch 88/10000, Batch 100/176, Loss: 0.9987\n",
            "Epoch 88 completed. Average Loss: 0.9636\n",
            "Epoch 89/10000, Batch 0/176, Loss: 0.8695\n",
            "Epoch 89/10000, Batch 100/176, Loss: 0.9878\n",
            "Epoch 89 completed. Average Loss: 0.9491\n",
            "Epoch 90/10000, Batch 0/176, Loss: 0.9120\n",
            "Epoch 90/10000, Batch 100/176, Loss: 0.8908\n",
            "Epoch 90 completed. Average Loss: 0.9397\n",
            "Epoch 91/10000, Batch 0/176, Loss: 0.8416\n",
            "Epoch 91/10000, Batch 100/176, Loss: 0.9984\n",
            "Validation Loss at iteration 16000: 10.6997\n",
            "Epoch 91 completed. Average Loss: 0.9312\n",
            "Epoch 92/10000, Batch 0/176, Loss: 0.8590\n",
            "Epoch 92/10000, Batch 100/176, Loss: 0.9035\n",
            "Epoch 92 completed. Average Loss: 0.9212\n",
            "Epoch 93/10000, Batch 0/176, Loss: 0.8043\n",
            "Epoch 93/10000, Batch 100/176, Loss: 0.9465\n",
            "Epoch 93 completed. Average Loss: 0.9101\n",
            "Epoch 94/10000, Batch 0/176, Loss: 0.8233\n",
            "Epoch 94/10000, Batch 100/176, Loss: 0.9178\n",
            "Epoch 94 completed. Average Loss: 0.9017\n",
            "Epoch 95/10000, Batch 0/176, Loss: 0.8205\n",
            "Epoch 95/10000, Batch 100/176, Loss: 0.9314\n",
            "Epoch 95 completed. Average Loss: 0.8909\n",
            "Epoch 96/10000, Batch 0/176, Loss: 0.8847\n",
            "Epoch 96/10000, Batch 100/176, Loss: 0.9280\n",
            "Epoch 96 completed. Average Loss: 0.8845\n",
            "Epoch 97/10000, Batch 0/176, Loss: 0.7979\n",
            "Epoch 97/10000, Batch 100/176, Loss: 0.9404\n",
            "Validation Loss at iteration 17000: 10.9093\n",
            "Epoch 97 completed. Average Loss: 0.8737\n",
            "Epoch 98/10000, Batch 0/176, Loss: 0.8050\n",
            "Epoch 98/10000, Batch 100/176, Loss: 0.9018\n",
            "Epoch 98 completed. Average Loss: 0.8637\n",
            "Epoch 99/10000, Batch 0/176, Loss: 0.8245\n",
            "Epoch 99/10000, Batch 100/176, Loss: 0.8756\n",
            "Epoch 99 completed. Average Loss: 0.8563\n",
            "Epoch 100/10000, Batch 0/176, Loss: 0.8125\n",
            "Epoch 100/10000, Batch 100/176, Loss: 0.8402\n",
            "Epoch 100 completed. Average Loss: 0.8459\n",
            "Epoch 101/10000, Batch 0/176, Loss: 0.7816\n",
            "Epoch 101/10000, Batch 100/176, Loss: 0.8058\n",
            "Epoch 101 completed. Average Loss: 0.8378\n",
            "Epoch 102/10000, Batch 0/176, Loss: 0.7891\n",
            "Epoch 102/10000, Batch 100/176, Loss: 0.8113\n",
            "Epoch 102 completed. Average Loss: 0.8319\n",
            "Epoch 103/10000, Batch 0/176, Loss: 0.8166\n",
            "Validation Loss at iteration 18000: 11.1938\n",
            "Epoch 103/10000, Batch 100/176, Loss: 0.8292\n",
            "Epoch 103 completed. Average Loss: 0.8231\n",
            "Epoch 104/10000, Batch 0/176, Loss: 0.7603\n",
            "Epoch 104/10000, Batch 100/176, Loss: 0.8073\n",
            "Epoch 104 completed. Average Loss: 0.8132\n",
            "Epoch 105/10000, Batch 0/176, Loss: 0.7584\n",
            "Epoch 105/10000, Batch 100/176, Loss: 0.8120\n",
            "Epoch 105 completed. Average Loss: 0.8062\n",
            "Epoch 106/10000, Batch 0/176, Loss: 0.7605\n",
            "Epoch 106/10000, Batch 100/176, Loss: 0.8153\n",
            "Epoch 106 completed. Average Loss: 0.7980\n",
            "Epoch 107/10000, Batch 0/176, Loss: 0.7103\n",
            "Epoch 107/10000, Batch 100/176, Loss: 0.7836\n",
            "Epoch 107 completed. Average Loss: 0.7925\n",
            "Epoch 108/10000, Batch 0/176, Loss: 0.7466\n",
            "Epoch 108/10000, Batch 100/176, Loss: 0.7776\n",
            "Validation Loss at iteration 19000: 11.4081\n",
            "Epoch 108 completed. Average Loss: 0.7830\n",
            "Epoch 109/10000, Batch 0/176, Loss: 0.7311\n",
            "Epoch 109/10000, Batch 100/176, Loss: 0.8255\n",
            "Epoch 109 completed. Average Loss: 0.7743\n",
            "Epoch 110/10000, Batch 0/176, Loss: 0.7003\n",
            "Epoch 110/10000, Batch 100/176, Loss: 0.8056\n",
            "Epoch 110 completed. Average Loss: 0.7695\n",
            "Epoch 111/10000, Batch 0/176, Loss: 0.7201\n",
            "Epoch 111/10000, Batch 100/176, Loss: 0.7354\n",
            "Epoch 111 completed. Average Loss: 0.7618\n",
            "Epoch 112/10000, Batch 0/176, Loss: 0.6982\n",
            "Epoch 112/10000, Batch 100/176, Loss: 0.7434\n",
            "Epoch 112 completed. Average Loss: 0.7564\n",
            "Epoch 113/10000, Batch 0/176, Loss: 0.7425\n",
            "Epoch 113/10000, Batch 100/176, Loss: 0.7430\n",
            "Epoch 113 completed. Average Loss: 0.7490\n",
            "Epoch 114/10000, Batch 0/176, Loss: 0.7035\n",
            "Epoch 114/10000, Batch 100/176, Loss: 0.7349\n",
            "Validation Loss at iteration 20000: 11.6441\n",
            "Epoch 114 completed. Average Loss: 0.7406\n",
            "Epoch 115/10000, Batch 0/176, Loss: 0.6916\n",
            "Epoch 115/10000, Batch 100/176, Loss: 0.7173\n",
            "Epoch 115 completed. Average Loss: 0.7354\n",
            "Epoch 116/10000, Batch 0/176, Loss: 0.7482\n",
            "Epoch 116/10000, Batch 100/176, Loss: 0.7562\n",
            "Epoch 116 completed. Average Loss: 0.7317\n",
            "Epoch 117/10000, Batch 0/176, Loss: 0.6718\n",
            "Epoch 117/10000, Batch 100/176, Loss: 0.6927\n",
            "Epoch 117 completed. Average Loss: 0.7189\n",
            "Epoch 118/10000, Batch 0/176, Loss: 0.6859\n",
            "Epoch 118/10000, Batch 100/176, Loss: 0.7566\n",
            "Epoch 118 completed. Average Loss: 0.7184\n",
            "Epoch 119/10000, Batch 0/176, Loss: 0.6193\n",
            "Epoch 119/10000, Batch 100/176, Loss: 0.7153\n",
            "Epoch 119 completed. Average Loss: 0.7112\n",
            "Epoch 120/10000, Batch 0/176, Loss: 0.6687\n",
            "Validation Loss at iteration 21000: 11.8285\n",
            "Epoch 120/10000, Batch 100/176, Loss: 0.6902\n",
            "Epoch 120 completed. Average Loss: 0.7037\n",
            "Epoch 121/10000, Batch 0/176, Loss: 0.6453\n",
            "Epoch 121/10000, Batch 100/176, Loss: 0.6871\n",
            "Epoch 121 completed. Average Loss: 0.7001\n",
            "Epoch 122/10000, Batch 0/176, Loss: 0.6483\n",
            "Epoch 122/10000, Batch 100/176, Loss: 0.6736\n",
            "Epoch 122 completed. Average Loss: 0.6914\n",
            "Epoch 123/10000, Batch 0/176, Loss: 0.6361\n",
            "Epoch 123/10000, Batch 100/176, Loss: 0.6803\n",
            "Epoch 123 completed. Average Loss: 0.6874\n",
            "Epoch 124/10000, Batch 0/176, Loss: 0.6687\n",
            "Epoch 124/10000, Batch 100/176, Loss: 0.6836\n",
            "Epoch 124 completed. Average Loss: 0.6810\n",
            "Epoch 125/10000, Batch 0/176, Loss: 0.6186\n",
            "Epoch 125/10000, Batch 100/176, Loss: 0.6975\n",
            "Validation Loss at iteration 22000: 12.0475\n",
            "Epoch 125 completed. Average Loss: 0.6762\n",
            "Epoch 126/10000, Batch 0/176, Loss: 0.6540\n",
            "Epoch 126/10000, Batch 100/176, Loss: 0.7020\n",
            "Epoch 126 completed. Average Loss: 0.6691\n",
            "Epoch 127/10000, Batch 0/176, Loss: 0.6369\n",
            "Epoch 127/10000, Batch 100/176, Loss: 0.6627\n",
            "Epoch 127 completed. Average Loss: 0.6647\n",
            "Epoch 128/10000, Batch 0/176, Loss: 0.6091\n",
            "Epoch 128/10000, Batch 100/176, Loss: 0.7014\n",
            "Epoch 128 completed. Average Loss: 0.6587\n",
            "Epoch 129/10000, Batch 0/176, Loss: 0.6203\n",
            "Epoch 129/10000, Batch 100/176, Loss: 0.6906\n",
            "Epoch 129 completed. Average Loss: 0.6516\n",
            "Epoch 130/10000, Batch 0/176, Loss: 0.6343\n",
            "Epoch 130/10000, Batch 100/176, Loss: 0.6745\n",
            "Epoch 130 completed. Average Loss: 0.6475\n",
            "Epoch 131/10000, Batch 0/176, Loss: 0.5862\n",
            "Epoch 131/10000, Batch 100/176, Loss: 0.6740\n",
            "Validation Loss at iteration 23000: 12.1814\n",
            "Epoch 131 completed. Average Loss: 0.6452\n",
            "Epoch 132/10000, Batch 0/176, Loss: 0.6219\n",
            "Epoch 132/10000, Batch 100/176, Loss: 0.6222\n",
            "Epoch 132 completed. Average Loss: 0.6411\n",
            "Epoch 133/10000, Batch 0/176, Loss: 0.5810\n",
            "Epoch 133/10000, Batch 100/176, Loss: 0.6219\n",
            "Epoch 133 completed. Average Loss: 0.6362\n",
            "Epoch 134/10000, Batch 0/176, Loss: 0.5891\n",
            "Epoch 134/10000, Batch 100/176, Loss: 0.6312\n",
            "Epoch 134 completed. Average Loss: 0.6308\n",
            "Epoch 135/10000, Batch 0/176, Loss: 0.5843\n",
            "Epoch 135/10000, Batch 100/176, Loss: 0.6632\n",
            "Epoch 135 completed. Average Loss: 0.6238\n",
            "Epoch 136/10000, Batch 0/176, Loss: 0.6532\n",
            "Epoch 136/10000, Batch 100/176, Loss: 0.6125\n",
            "Epoch 136 completed. Average Loss: 0.6191\n",
            "Epoch 137/10000, Batch 0/176, Loss: 0.5872\n",
            "Validation Loss at iteration 24000: 12.4589\n",
            "Epoch 137/10000, Batch 100/176, Loss: 0.6392\n",
            "Epoch 137 completed. Average Loss: 0.6132\n",
            "Epoch 138/10000, Batch 0/176, Loss: 0.5930\n",
            "Epoch 138/10000, Batch 100/176, Loss: 0.6419\n",
            "Epoch 138 completed. Average Loss: 0.6126\n",
            "Epoch 139/10000, Batch 0/176, Loss: 0.5555\n",
            "Epoch 139/10000, Batch 100/176, Loss: 0.6061\n",
            "Epoch 139 completed. Average Loss: 0.6061\n",
            "Epoch 140/10000, Batch 0/176, Loss: 0.5849\n",
            "Epoch 140/10000, Batch 100/176, Loss: 0.6057\n",
            "Epoch 140 completed. Average Loss: 0.6010\n",
            "Epoch 141/10000, Batch 0/176, Loss: 0.5783\n",
            "Epoch 141/10000, Batch 100/176, Loss: 0.6021\n",
            "Epoch 141 completed. Average Loss: 0.5957\n",
            "Epoch 142/10000, Batch 0/176, Loss: 0.5430\n",
            "Epoch 142/10000, Batch 100/176, Loss: 0.5987\n",
            "Epoch 142 completed. Average Loss: 0.5921\n",
            "Epoch 143/10000, Batch 0/176, Loss: 0.5843\n",
            "Validation Loss at iteration 25000: 12.5996\n",
            "Epoch 143/10000, Batch 100/176, Loss: 0.5686\n",
            "Epoch 143 completed. Average Loss: 0.5916\n",
            "Epoch 144/10000, Batch 0/176, Loss: 0.5211\n",
            "Epoch 144/10000, Batch 100/176, Loss: 0.6013\n",
            "Epoch 144 completed. Average Loss: 0.5819\n",
            "Epoch 145/10000, Batch 0/176, Loss: 0.5094\n",
            "Epoch 145/10000, Batch 100/176, Loss: 0.5839\n",
            "Epoch 145 completed. Average Loss: 0.5812\n",
            "Epoch 146/10000, Batch 0/176, Loss: 0.5495\n",
            "Epoch 146/10000, Batch 100/176, Loss: 0.5995\n",
            "Epoch 146 completed. Average Loss: 0.5764\n",
            "Epoch 147/10000, Batch 0/176, Loss: 0.5502\n",
            "Epoch 147/10000, Batch 100/176, Loss: 0.6194\n",
            "Epoch 147 completed. Average Loss: 0.5714\n",
            "Epoch 148/10000, Batch 0/176, Loss: 0.5228\n",
            "Epoch 148/10000, Batch 100/176, Loss: 0.5538\n",
            "Validation Loss at iteration 26000: 12.7712\n",
            "Epoch 148 completed. Average Loss: 0.5666\n",
            "Epoch 149/10000, Batch 0/176, Loss: 0.5520\n",
            "Epoch 149/10000, Batch 100/176, Loss: 0.5897\n",
            "Epoch 149 completed. Average Loss: 0.5650\n",
            "Epoch 150/10000, Batch 0/176, Loss: 0.5133\n",
            "Epoch 150/10000, Batch 100/176, Loss: 0.5445\n",
            "Epoch 150 completed. Average Loss: 0.5603\n",
            "Epoch 151/10000, Batch 0/176, Loss: 0.5426\n",
            "Epoch 151/10000, Batch 100/176, Loss: 0.5613\n",
            "Epoch 151 completed. Average Loss: 0.5571\n",
            "Epoch 152/10000, Batch 0/176, Loss: 0.5382\n",
            "Epoch 152/10000, Batch 100/176, Loss: 0.5552\n",
            "Epoch 152 completed. Average Loss: 0.5530\n",
            "Epoch 153/10000, Batch 0/176, Loss: 0.4917\n",
            "Epoch 153/10000, Batch 100/176, Loss: 0.5374\n",
            "Epoch 153 completed. Average Loss: 0.5504\n",
            "Epoch 154/10000, Batch 0/176, Loss: 0.4943\n",
            "Validation Loss at iteration 27000: 12.9437\n",
            "Epoch 154/10000, Batch 100/176, Loss: 0.5789\n",
            "Epoch 154 completed. Average Loss: 0.5479\n",
            "Epoch 155/10000, Batch 0/176, Loss: 0.5260\n",
            "Epoch 155/10000, Batch 100/176, Loss: 0.5471\n",
            "Epoch 155 completed. Average Loss: 0.5452\n",
            "Epoch 156/10000, Batch 0/176, Loss: 0.5073\n",
            "Epoch 156/10000, Batch 100/176, Loss: 0.5330\n",
            "Epoch 156 completed. Average Loss: 0.5417\n",
            "Epoch 157/10000, Batch 0/176, Loss: 0.5046\n",
            "Epoch 157/10000, Batch 100/176, Loss: 0.5223\n",
            "Epoch 157 completed. Average Loss: 0.5341\n",
            "Epoch 158/10000, Batch 0/176, Loss: 0.4845\n",
            "Epoch 158/10000, Batch 100/176, Loss: 0.5147\n",
            "Epoch 158 completed. Average Loss: 0.5314\n",
            "Epoch 159/10000, Batch 0/176, Loss: 0.4671\n",
            "Epoch 159/10000, Batch 100/176, Loss: 0.5275\n",
            "Epoch 159 completed. Average Loss: 0.5295\n",
            "Epoch 160/10000, Batch 0/176, Loss: 0.4753\n",
            "Validation Loss at iteration 28000: 13.0595\n",
            "Epoch 160/10000, Batch 100/176, Loss: 0.5405\n",
            "Epoch 160 completed. Average Loss: 0.5234\n",
            "Epoch 161/10000, Batch 0/176, Loss: 0.4878\n",
            "Epoch 161/10000, Batch 100/176, Loss: 0.5361\n",
            "Epoch 161 completed. Average Loss: 0.5225\n",
            "Epoch 162/10000, Batch 0/176, Loss: 0.5074\n",
            "Epoch 162/10000, Batch 100/176, Loss: 0.5064\n",
            "Epoch 162 completed. Average Loss: 0.5196\n",
            "Epoch 163/10000, Batch 0/176, Loss: 0.4751\n",
            "Epoch 163/10000, Batch 100/176, Loss: 0.5027\n",
            "Epoch 163 completed. Average Loss: 0.5150\n",
            "Epoch 164/10000, Batch 0/176, Loss: 0.4610\n",
            "Epoch 164/10000, Batch 100/176, Loss: 0.4872\n",
            "Epoch 164 completed. Average Loss: 0.5132\n",
            "Epoch 165/10000, Batch 0/176, Loss: 0.4589\n",
            "Epoch 165/10000, Batch 100/176, Loss: 0.5118\n",
            "Validation Loss at iteration 29000: 13.1985\n",
            "Epoch 165 completed. Average Loss: 0.5135\n",
            "Epoch 166/10000, Batch 0/176, Loss: 0.4855\n",
            "Epoch 166/10000, Batch 100/176, Loss: 0.5175\n",
            "Epoch 166 completed. Average Loss: 0.5061\n",
            "Epoch 167/10000, Batch 0/176, Loss: 0.4692\n",
            "Epoch 167/10000, Batch 100/176, Loss: 0.5004\n",
            "Epoch 167 completed. Average Loss: 0.5033\n",
            "Epoch 168/10000, Batch 0/176, Loss: 0.4880\n",
            "Epoch 168/10000, Batch 100/176, Loss: 0.4996\n",
            "Epoch 168 completed. Average Loss: 0.5009\n",
            "Epoch 169/10000, Batch 0/176, Loss: 0.4640\n",
            "Epoch 169/10000, Batch 100/176, Loss: 0.4959\n",
            "Epoch 169 completed. Average Loss: 0.4984\n",
            "Epoch 170/10000, Batch 0/176, Loss: 0.4594\n",
            "Epoch 170/10000, Batch 100/176, Loss: 0.4811\n",
            "Epoch 170 completed. Average Loss: 0.4963\n",
            "Epoch 171/10000, Batch 0/176, Loss: 0.4497\n",
            "Validation Loss at iteration 30000: 13.3377\n",
            "Epoch 171/10000, Batch 100/176, Loss: 0.4741\n",
            "Epoch 171 completed. Average Loss: 0.4936\n",
            "Epoch 172/10000, Batch 0/176, Loss: 0.4519\n",
            "Epoch 172/10000, Batch 100/176, Loss: 0.5164\n",
            "Epoch 172 completed. Average Loss: 0.4896\n",
            "Epoch 173/10000, Batch 0/176, Loss: 0.4406\n",
            "Epoch 173/10000, Batch 100/176, Loss: 0.5132\n",
            "Epoch 173 completed. Average Loss: 0.4892\n",
            "Epoch 174/10000, Batch 0/176, Loss: 0.4516\n",
            "Epoch 174/10000, Batch 100/176, Loss: 0.4832\n",
            "Epoch 174 completed. Average Loss: 0.4851\n",
            "Epoch 175/10000, Batch 0/176, Loss: 0.4591\n",
            "Epoch 175/10000, Batch 100/176, Loss: 0.4896\n",
            "Epoch 175 completed. Average Loss: 0.4826\n",
            "Epoch 176/10000, Batch 0/176, Loss: 0.4934\n",
            "Epoch 176/10000, Batch 100/176, Loss: 0.4985\n",
            "Epoch 176 completed. Average Loss: 0.4791\n",
            "Epoch 177/10000, Batch 0/176, Loss: 0.4086\n",
            "Validation Loss at iteration 31000: 13.4738\n",
            "Epoch 177/10000, Batch 100/176, Loss: 0.4934\n",
            "Epoch 177 completed. Average Loss: 0.4759\n",
            "Epoch 178/10000, Batch 0/176, Loss: 0.4426\n",
            "Epoch 178/10000, Batch 100/176, Loss: 0.4885\n",
            "Epoch 178 completed. Average Loss: 0.4720\n",
            "Epoch 179/10000, Batch 0/176, Loss: 0.4525\n",
            "Epoch 179/10000, Batch 100/176, Loss: 0.4835\n",
            "Epoch 179 completed. Average Loss: 0.4723\n",
            "Epoch 180/10000, Batch 0/176, Loss: 0.4485\n",
            "Epoch 180/10000, Batch 100/176, Loss: 0.4446\n",
            "Epoch 180 completed. Average Loss: 0.4667\n",
            "Epoch 181/10000, Batch 0/176, Loss: 0.4361\n",
            "Epoch 181/10000, Batch 100/176, Loss: 0.4799\n",
            "Epoch 181 completed. Average Loss: 0.4680\n",
            "Epoch 182/10000, Batch 0/176, Loss: 0.4263\n",
            "Epoch 182/10000, Batch 100/176, Loss: 0.4509\n",
            "Validation Loss at iteration 32000: 13.6249\n",
            "Epoch 182 completed. Average Loss: 0.4641\n",
            "Epoch 183/10000, Batch 0/176, Loss: 0.4121\n",
            "Epoch 183/10000, Batch 100/176, Loss: 0.4750\n",
            "Epoch 183 completed. Average Loss: 0.4598\n",
            "Epoch 184/10000, Batch 0/176, Loss: 0.4092\n",
            "Epoch 184/10000, Batch 100/176, Loss: 0.4479\n",
            "Epoch 184 completed. Average Loss: 0.4581\n",
            "Epoch 185/10000, Batch 0/176, Loss: 0.4223\n",
            "Epoch 185/10000, Batch 100/176, Loss: 0.4644\n",
            "Epoch 185 completed. Average Loss: 0.4554\n",
            "Epoch 186/10000, Batch 0/176, Loss: 0.4274\n",
            "Epoch 186/10000, Batch 100/176, Loss: 0.4510\n",
            "Epoch 186 completed. Average Loss: 0.4521\n",
            "Epoch 187/10000, Batch 0/176, Loss: 0.3978\n",
            "Epoch 187/10000, Batch 100/176, Loss: 0.4456\n",
            "Epoch 187 completed. Average Loss: 0.4511\n",
            "Epoch 188/10000, Batch 0/176, Loss: 0.3894\n",
            "Validation Loss at iteration 33000: 13.7384\n",
            "Epoch 188/10000, Batch 100/176, Loss: 0.4178\n",
            "Epoch 188 completed. Average Loss: 0.4489\n",
            "Epoch 189/10000, Batch 0/176, Loss: 0.4199\n",
            "Epoch 189/10000, Batch 100/176, Loss: 0.4825\n",
            "Epoch 189 completed. Average Loss: 0.4459\n",
            "Epoch 190/10000, Batch 0/176, Loss: 0.4373\n",
            "Epoch 190/10000, Batch 100/176, Loss: 0.4306\n",
            "Epoch 190 completed. Average Loss: 0.4462\n",
            "Epoch 191/10000, Batch 0/176, Loss: 0.4306\n",
            "Epoch 191/10000, Batch 100/176, Loss: 0.4527\n",
            "Epoch 191 completed. Average Loss: 0.4431\n",
            "Epoch 192/10000, Batch 0/176, Loss: 0.4015\n",
            "Epoch 192/10000, Batch 100/176, Loss: 0.4737\n",
            "Epoch 192 completed. Average Loss: 0.4395\n",
            "Epoch 193/10000, Batch 0/176, Loss: 0.4242\n",
            "Epoch 193/10000, Batch 100/176, Loss: 0.4326\n",
            "Epoch 193 completed. Average Loss: 0.4371\n",
            "Epoch 194/10000, Batch 0/176, Loss: 0.4045\n",
            "Validation Loss at iteration 34000: 13.9014\n",
            "Epoch 194/10000, Batch 100/176, Loss: 0.4828\n",
            "Epoch 194 completed. Average Loss: 0.4345\n",
            "Epoch 195/10000, Batch 0/176, Loss: 0.3991\n",
            "Epoch 195/10000, Batch 100/176, Loss: 0.4444\n",
            "Epoch 195 completed. Average Loss: 0.4343\n",
            "Epoch 196/10000, Batch 0/176, Loss: 0.4037\n",
            "Epoch 196/10000, Batch 100/176, Loss: 0.4431\n",
            "Epoch 196 completed. Average Loss: 0.4342\n",
            "Epoch 197/10000, Batch 0/176, Loss: 0.4130\n",
            "Epoch 197/10000, Batch 100/176, Loss: 0.4300\n",
            "Epoch 197 completed. Average Loss: 0.4293\n",
            "Epoch 198/10000, Batch 0/176, Loss: 0.4190\n",
            "Epoch 198/10000, Batch 100/176, Loss: 0.4250\n",
            "Epoch 198 completed. Average Loss: 0.4284\n",
            "Epoch 199/10000, Batch 0/176, Loss: 0.3857\n",
            "Epoch 199/10000, Batch 100/176, Loss: 0.4421\n",
            "Validation Loss at iteration 35000: 14.0108\n",
            "Epoch 199 completed. Average Loss: 0.4243\n",
            "Epoch 200/10000, Batch 0/176, Loss: 0.3904\n",
            "Epoch 200/10000, Batch 100/176, Loss: 0.4305\n",
            "Epoch 200 completed. Average Loss: 0.4252\n",
            "Epoch 201/10000, Batch 0/176, Loss: 0.3930\n",
            "Epoch 201/10000, Batch 100/176, Loss: 0.4112\n",
            "Epoch 201 completed. Average Loss: 0.4187\n",
            "Epoch 202/10000, Batch 0/176, Loss: 0.3689\n",
            "Epoch 202/10000, Batch 100/176, Loss: 0.4330\n",
            "Epoch 202 completed. Average Loss: 0.4205\n",
            "Epoch 203/10000, Batch 0/176, Loss: 0.3773\n",
            "Epoch 203/10000, Batch 100/176, Loss: 0.4384\n",
            "Epoch 203 completed. Average Loss: 0.4192\n",
            "Epoch 204/10000, Batch 0/176, Loss: 0.3880\n",
            "Epoch 204/10000, Batch 100/176, Loss: 0.4233\n",
            "Epoch 204 completed. Average Loss: 0.4165\n",
            "Epoch 205/10000, Batch 0/176, Loss: 0.4366\n",
            "Validation Loss at iteration 36000: 14.0983\n",
            "Epoch 205/10000, Batch 100/176, Loss: 0.3973\n",
            "Epoch 205 completed. Average Loss: 0.4141\n",
            "Epoch 206/10000, Batch 0/176, Loss: 0.3712\n",
            "Epoch 206/10000, Batch 100/176, Loss: 0.4280\n",
            "Epoch 206 completed. Average Loss: 0.4121\n",
            "Epoch 207/10000, Batch 0/176, Loss: 0.3969\n",
            "Epoch 207/10000, Batch 100/176, Loss: 0.4117\n",
            "Epoch 207 completed. Average Loss: 0.4098\n",
            "Epoch 208/10000, Batch 0/176, Loss: 0.3921\n",
            "Epoch 208/10000, Batch 100/176, Loss: 0.4257\n",
            "Epoch 208 completed. Average Loss: 0.4087\n",
            "Epoch 209/10000, Batch 0/176, Loss: 0.3672\n",
            "Epoch 209/10000, Batch 100/176, Loss: 0.4413\n",
            "Epoch 209 completed. Average Loss: 0.4068\n",
            "Epoch 210/10000, Batch 0/176, Loss: 0.3901\n",
            "Epoch 210/10000, Batch 100/176, Loss: 0.4024\n",
            "Epoch 210 completed. Average Loss: 0.4034\n",
            "Epoch 211/10000, Batch 0/176, Loss: 0.3643\n",
            "Validation Loss at iteration 37000: 14.1832\n",
            "Epoch 211/10000, Batch 100/176, Loss: 0.4234\n",
            "Epoch 211 completed. Average Loss: 0.4027\n",
            "Epoch 212/10000, Batch 0/176, Loss: 0.3755\n",
            "Epoch 212/10000, Batch 100/176, Loss: 0.4346\n",
            "Epoch 212 completed. Average Loss: 0.4045\n",
            "Epoch 213/10000, Batch 0/176, Loss: 0.3355\n",
            "Epoch 213/10000, Batch 100/176, Loss: 0.4139\n",
            "Epoch 213 completed. Average Loss: 0.3990\n",
            "Epoch 214/10000, Batch 0/176, Loss: 0.3601\n",
            "Epoch 214/10000, Batch 100/176, Loss: 0.3892\n",
            "Epoch 214 completed. Average Loss: 0.3972\n",
            "Epoch 215/10000, Batch 0/176, Loss: 0.3519\n",
            "Epoch 215/10000, Batch 100/176, Loss: 0.3643\n",
            "Epoch 215 completed. Average Loss: 0.3966\n",
            "Epoch 216/10000, Batch 0/176, Loss: 0.3285\n",
            "Epoch 216/10000, Batch 100/176, Loss: 0.4059\n",
            "Validation Loss at iteration 38000: 14.3190\n",
            "Epoch 216 completed. Average Loss: 0.3930\n",
            "Epoch 217/10000, Batch 0/176, Loss: 0.3393\n",
            "Epoch 217/10000, Batch 100/176, Loss: 0.4228\n",
            "Epoch 217 completed. Average Loss: 0.3930\n",
            "Epoch 218/10000, Batch 0/176, Loss: 0.3540\n",
            "Epoch 218/10000, Batch 100/176, Loss: 0.3777\n",
            "Epoch 218 completed. Average Loss: 0.3917\n",
            "Epoch 219/10000, Batch 0/176, Loss: 0.3586\n",
            "Epoch 219/10000, Batch 100/176, Loss: 0.3839\n",
            "Epoch 219 completed. Average Loss: 0.3873\n",
            "Epoch 220/10000, Batch 0/176, Loss: 0.3840\n",
            "Epoch 220/10000, Batch 100/176, Loss: 0.4176\n",
            "Epoch 220 completed. Average Loss: 0.3882\n",
            "Epoch 221/10000, Batch 0/176, Loss: 0.3519\n",
            "Epoch 221/10000, Batch 100/176, Loss: 0.3881\n",
            "Epoch 221 completed. Average Loss: 0.3858\n",
            "Epoch 222/10000, Batch 0/176, Loss: 0.3529\n",
            "Epoch 222/10000, Batch 100/176, Loss: 0.4064\n",
            "Validation Loss at iteration 39000: 14.3387\n",
            "Epoch 222 completed. Average Loss: 0.3861\n",
            "Epoch 223/10000, Batch 0/176, Loss: 0.3466\n",
            "Epoch 223/10000, Batch 100/176, Loss: 0.4166\n",
            "Epoch 223 completed. Average Loss: 0.3830\n",
            "Epoch 224/10000, Batch 0/176, Loss: 0.3424\n",
            "Epoch 224/10000, Batch 100/176, Loss: 0.3856\n",
            "Epoch 224 completed. Average Loss: 0.3805\n",
            "Epoch 225/10000, Batch 0/176, Loss: 0.3405\n",
            "Epoch 225/10000, Batch 100/176, Loss: 0.3721\n",
            "Epoch 225 completed. Average Loss: 0.3780\n",
            "Epoch 226/10000, Batch 0/176, Loss: 0.3629\n",
            "Epoch 226/10000, Batch 100/176, Loss: 0.4034\n",
            "Epoch 226 completed. Average Loss: 0.3766\n",
            "Epoch 227/10000, Batch 0/176, Loss: 0.3403\n",
            "Epoch 227/10000, Batch 100/176, Loss: 0.3756\n",
            "Epoch 227 completed. Average Loss: 0.3772\n",
            "Epoch 228/10000, Batch 0/176, Loss: 0.3534\n",
            "Validation Loss at iteration 40000: 14.4312\n",
            "Epoch 228/10000, Batch 100/176, Loss: 0.3683\n",
            "Epoch 228 completed. Average Loss: 0.3744\n",
            "Epoch 229/10000, Batch 0/176, Loss: 0.3426\n",
            "Epoch 229/10000, Batch 100/176, Loss: 0.3966\n",
            "Epoch 229 completed. Average Loss: 0.3736\n",
            "Epoch 230/10000, Batch 0/176, Loss: 0.3553\n",
            "Epoch 230/10000, Batch 100/176, Loss: 0.3459\n",
            "Epoch 230 completed. Average Loss: 0.3740\n",
            "Epoch 231/10000, Batch 0/176, Loss: 0.3462\n",
            "Epoch 231/10000, Batch 100/176, Loss: 0.4040\n",
            "Epoch 231 completed. Average Loss: 0.3701\n",
            "Epoch 232/10000, Batch 0/176, Loss: 0.3621\n",
            "Epoch 232/10000, Batch 100/176, Loss: 0.3827\n",
            "Epoch 232 completed. Average Loss: 0.3698\n",
            "Epoch 233/10000, Batch 0/176, Loss: 0.3642\n",
            "Epoch 233/10000, Batch 100/176, Loss: 0.3965\n",
            "Validation Loss at iteration 41000: 14.5402\n",
            "Epoch 233 completed. Average Loss: 0.3682\n",
            "Epoch 234/10000, Batch 0/176, Loss: 0.3461\n",
            "Epoch 234/10000, Batch 100/176, Loss: 0.3868\n",
            "Epoch 234 completed. Average Loss: 0.3671\n",
            "Epoch 235/10000, Batch 0/176, Loss: 0.3585\n",
            "Epoch 235/10000, Batch 100/176, Loss: 0.3771\n",
            "Epoch 235 completed. Average Loss: 0.3651\n",
            "Epoch 236/10000, Batch 0/176, Loss: 0.3528\n",
            "Epoch 236/10000, Batch 100/176, Loss: 0.3473\n",
            "Epoch 236 completed. Average Loss: 0.3640\n",
            "Epoch 237/10000, Batch 0/176, Loss: 0.3108\n",
            "Epoch 237/10000, Batch 100/176, Loss: 0.3538\n",
            "Epoch 237 completed. Average Loss: 0.3601\n",
            "Epoch 238/10000, Batch 0/176, Loss: 0.3164\n",
            "Epoch 238/10000, Batch 100/176, Loss: 0.3727\n",
            "Epoch 238 completed. Average Loss: 0.3601\n",
            "Epoch 239/10000, Batch 0/176, Loss: 0.3162\n",
            "Epoch 239/10000, Batch 100/176, Loss: 0.3459\n",
            "Validation Loss at iteration 42000: 14.6714\n",
            "Epoch 239 completed. Average Loss: 0.3581\n",
            "Epoch 240/10000, Batch 0/176, Loss: 0.3376\n",
            "Epoch 240/10000, Batch 100/176, Loss: 0.3715\n",
            "Epoch 240 completed. Average Loss: 0.3595\n",
            "Epoch 241/10000, Batch 0/176, Loss: 0.3401\n",
            "Epoch 241/10000, Batch 100/176, Loss: 0.3509\n",
            "Epoch 241 completed. Average Loss: 0.3551\n",
            "Epoch 242/10000, Batch 0/176, Loss: 0.3155\n",
            "Epoch 242/10000, Batch 100/176, Loss: 0.3482\n",
            "Epoch 242 completed. Average Loss: 0.3570\n",
            "Epoch 243/10000, Batch 0/176, Loss: 0.3152\n",
            "Epoch 243/10000, Batch 100/176, Loss: 0.3663\n",
            "Epoch 243 completed. Average Loss: 0.3538\n",
            "Epoch 244/10000, Batch 0/176, Loss: 0.3419\n",
            "Epoch 244/10000, Batch 100/176, Loss: 0.3616\n",
            "Epoch 244 completed. Average Loss: 0.3510\n",
            "Epoch 245/10000, Batch 0/176, Loss: 0.3119\n",
            "Validation Loss at iteration 43000: 14.7428\n",
            "Epoch 245/10000, Batch 100/176, Loss: 0.3742\n",
            "Epoch 245 completed. Average Loss: 0.3494\n",
            "Epoch 246/10000, Batch 0/176, Loss: 0.3264\n",
            "Epoch 246/10000, Batch 100/176, Loss: 0.3544\n",
            "Epoch 246 completed. Average Loss: 0.3493\n",
            "Epoch 247/10000, Batch 0/176, Loss: 0.3441\n",
            "Epoch 247/10000, Batch 100/176, Loss: 0.3719\n",
            "Epoch 247 completed. Average Loss: 0.3507\n",
            "Epoch 248/10000, Batch 0/176, Loss: 0.3370\n",
            "Epoch 248/10000, Batch 100/176, Loss: 0.3419\n",
            "Epoch 248 completed. Average Loss: 0.3466\n",
            "Epoch 249/10000, Batch 0/176, Loss: 0.3239\n",
            "Epoch 249/10000, Batch 100/176, Loss: 0.3548\n",
            "Epoch 249 completed. Average Loss: 0.3476\n",
            "Epoch 250/10000, Batch 0/176, Loss: 0.3266\n",
            "Epoch 250/10000, Batch 100/176, Loss: 0.3337\n",
            "Validation Loss at iteration 44000: 14.8427\n",
            "Epoch 250 completed. Average Loss: 0.3456\n",
            "Epoch 251/10000, Batch 0/176, Loss: 0.3223\n",
            "Epoch 251/10000, Batch 100/176, Loss: 0.3330\n",
            "Epoch 251 completed. Average Loss: 0.3448\n",
            "Epoch 252/10000, Batch 0/176, Loss: 0.3348\n",
            "Epoch 252/10000, Batch 100/176, Loss: 0.3303\n",
            "Epoch 252 completed. Average Loss: 0.3427\n",
            "Epoch 253/10000, Batch 0/176, Loss: 0.3279\n",
            "Epoch 253/10000, Batch 100/176, Loss: 0.3164\n",
            "Epoch 253 completed. Average Loss: 0.3388\n",
            "Epoch 254/10000, Batch 0/176, Loss: 0.3118\n",
            "Epoch 254/10000, Batch 100/176, Loss: 0.3260\n",
            "Epoch 254 completed. Average Loss: 0.3409\n",
            "Epoch 255/10000, Batch 0/176, Loss: 0.3413\n",
            "Epoch 255/10000, Batch 100/176, Loss: 0.3550\n",
            "Epoch 255 completed. Average Loss: 0.3398\n",
            "Epoch 256/10000, Batch 0/176, Loss: 0.2809\n",
            "Epoch 256/10000, Batch 100/176, Loss: 0.3534\n",
            "Validation Loss at iteration 45000: 14.8857\n",
            "Epoch 256 completed. Average Loss: 0.3380\n",
            "Epoch 257/10000, Batch 0/176, Loss: 0.3142\n",
            "Epoch 257/10000, Batch 100/176, Loss: 0.3612\n",
            "Epoch 257 completed. Average Loss: 0.3354\n",
            "Epoch 258/10000, Batch 0/176, Loss: 0.3122\n",
            "Epoch 258/10000, Batch 100/176, Loss: 0.3664\n",
            "Epoch 258 completed. Average Loss: 0.3349\n",
            "Epoch 259/10000, Batch 0/176, Loss: 0.2829\n",
            "Epoch 259/10000, Batch 100/176, Loss: 0.3278\n",
            "Epoch 259 completed. Average Loss: 0.3335\n",
            "Epoch 260/10000, Batch 0/176, Loss: 0.3060\n",
            "Epoch 260/10000, Batch 100/176, Loss: 0.3425\n",
            "Epoch 260 completed. Average Loss: 0.3323\n",
            "Epoch 261/10000, Batch 0/176, Loss: 0.3020\n",
            "Epoch 261/10000, Batch 100/176, Loss: 0.3196\n",
            "Epoch 261 completed. Average Loss: 0.3324\n",
            "Epoch 262/10000, Batch 0/176, Loss: 0.3113\n",
            "Validation Loss at iteration 46000: 15.0451\n",
            "Epoch 262/10000, Batch 100/176, Loss: 0.3212\n",
            "Epoch 262 completed. Average Loss: 0.3324\n",
            "Epoch 263/10000, Batch 0/176, Loss: 0.2870\n",
            "Epoch 263/10000, Batch 100/176, Loss: 0.3402\n",
            "Epoch 263 completed. Average Loss: 0.3310\n",
            "Epoch 264/10000, Batch 0/176, Loss: 0.3148\n",
            "Epoch 264/10000, Batch 100/176, Loss: 0.3171\n",
            "Epoch 264 completed. Average Loss: 0.3299\n",
            "Epoch 265/10000, Batch 0/176, Loss: 0.3341\n",
            "Epoch 265/10000, Batch 100/176, Loss: 0.3448\n",
            "Epoch 265 completed. Average Loss: 0.3277\n",
            "Epoch 266/10000, Batch 0/176, Loss: 0.2981\n",
            "Epoch 266/10000, Batch 100/176, Loss: 0.3391\n",
            "Epoch 266 completed. Average Loss: 0.3254\n",
            "Epoch 267/10000, Batch 0/176, Loss: 0.3235\n",
            "Epoch 267/10000, Batch 100/176, Loss: 0.3341\n",
            "Epoch 267 completed. Average Loss: 0.3286\n",
            "Epoch 268/10000, Batch 0/176, Loss: 0.3372\n",
            "Validation Loss at iteration 47000: 15.0161\n",
            "Epoch 268/10000, Batch 100/176, Loss: 0.3462\n",
            "Epoch 268 completed. Average Loss: 0.3238\n",
            "Epoch 269/10000, Batch 0/176, Loss: 0.3154\n",
            "Epoch 269/10000, Batch 100/176, Loss: 0.3233\n",
            "Epoch 269 completed. Average Loss: 0.3235\n",
            "Epoch 270/10000, Batch 0/176, Loss: 0.3223\n",
            "Epoch 270/10000, Batch 100/176, Loss: 0.3329\n",
            "Epoch 270 completed. Average Loss: 0.3225\n",
            "Epoch 271/10000, Batch 0/176, Loss: 0.2853\n",
            "Epoch 271/10000, Batch 100/176, Loss: 0.2977\n",
            "Epoch 271 completed. Average Loss: 0.3201\n",
            "Epoch 272/10000, Batch 0/176, Loss: 0.2866\n",
            "Epoch 272/10000, Batch 100/176, Loss: 0.3300\n",
            "Epoch 272 completed. Average Loss: 0.3182\n",
            "Epoch 273/10000, Batch 0/176, Loss: 0.2859\n",
            "Epoch 273/10000, Batch 100/176, Loss: 0.3220\n",
            "Validation Loss at iteration 48000: 15.1561\n",
            "Epoch 273 completed. Average Loss: 0.3203\n",
            "Epoch 274/10000, Batch 0/176, Loss: 0.3057\n",
            "Epoch 274/10000, Batch 100/176, Loss: 0.3395\n",
            "Epoch 274 completed. Average Loss: 0.3175\n",
            "Epoch 275/10000, Batch 0/176, Loss: 0.2792\n",
            "Epoch 275/10000, Batch 100/176, Loss: 0.3022\n",
            "Epoch 275 completed. Average Loss: 0.3164\n",
            "Epoch 276/10000, Batch 0/176, Loss: 0.2890\n",
            "Epoch 276/10000, Batch 100/176, Loss: 0.3232\n",
            "Epoch 276 completed. Average Loss: 0.3166\n",
            "Epoch 277/10000, Batch 0/176, Loss: 0.2802\n",
            "Epoch 277/10000, Batch 100/176, Loss: 0.3250\n",
            "Epoch 277 completed. Average Loss: 0.3134\n",
            "Epoch 278/10000, Batch 0/176, Loss: 0.3121\n",
            "Epoch 278/10000, Batch 100/176, Loss: 0.3413\n",
            "Epoch 278 completed. Average Loss: 0.3133\n",
            "Epoch 279/10000, Batch 0/176, Loss: 0.2861\n",
            "Validation Loss at iteration 49000: 15.2321\n",
            "Epoch 279/10000, Batch 100/176, Loss: 0.3215\n",
            "Epoch 279 completed. Average Loss: 0.3116\n",
            "Epoch 280/10000, Batch 0/176, Loss: 0.2968\n",
            "Epoch 280/10000, Batch 100/176, Loss: 0.3186\n",
            "Epoch 280 completed. Average Loss: 0.3115\n",
            "Epoch 281/10000, Batch 0/176, Loss: 0.3153\n",
            "Epoch 281/10000, Batch 100/176, Loss: 0.2921\n",
            "Epoch 281 completed. Average Loss: 0.3118\n",
            "Epoch 282/10000, Batch 0/176, Loss: 0.3026\n",
            "Epoch 282/10000, Batch 100/176, Loss: 0.3084\n",
            "Epoch 282 completed. Average Loss: 0.3109\n",
            "Epoch 283/10000, Batch 0/176, Loss: 0.2803\n",
            "Epoch 283/10000, Batch 100/176, Loss: 0.3195\n",
            "Epoch 283 completed. Average Loss: 0.3089\n",
            "Epoch 284/10000, Batch 0/176, Loss: 0.2678\n",
            "Epoch 284/10000, Batch 100/176, Loss: 0.2993\n",
            "Epoch 284 completed. Average Loss: 0.3077\n",
            "Epoch 285/10000, Batch 0/176, Loss: 0.2683\n",
            "Validation Loss at iteration 50000: 15.2886\n",
            "Epoch 285/10000, Batch 100/176, Loss: 0.3235\n",
            "Epoch 285 completed. Average Loss: 0.3071\n",
            "Epoch 286/10000, Batch 0/176, Loss: 0.2827\n",
            "Epoch 286/10000, Batch 100/176, Loss: 0.3106\n",
            "Epoch 286 completed. Average Loss: 0.3059\n",
            "Epoch 287/10000, Batch 0/176, Loss: 0.2756\n",
            "Epoch 287/10000, Batch 100/176, Loss: 0.2889\n",
            "Epoch 287 completed. Average Loss: 0.3073\n",
            "Epoch 288/10000, Batch 0/176, Loss: 0.2876\n",
            "Epoch 288/10000, Batch 100/176, Loss: 0.3203\n",
            "Epoch 288 completed. Average Loss: 0.3055\n",
            "Epoch 289/10000, Batch 0/176, Loss: 0.2961\n",
            "Epoch 289/10000, Batch 100/176, Loss: 0.3151\n",
            "Epoch 289 completed. Average Loss: 0.3066\n",
            "Epoch 290/10000, Batch 0/176, Loss: 0.2840\n",
            "Epoch 290/10000, Batch 100/176, Loss: 0.3207\n",
            "Validation Loss at iteration 51000: 15.3991\n",
            "Epoch 290 completed. Average Loss: 0.3026\n",
            "Epoch 291/10000, Batch 0/176, Loss: 0.2853\n",
            "Epoch 291/10000, Batch 100/176, Loss: 0.3020\n",
            "Epoch 291 completed. Average Loss: 0.3022\n",
            "Epoch 292/10000, Batch 0/176, Loss: 0.3016\n",
            "Epoch 292/10000, Batch 100/176, Loss: 0.3143\n",
            "Epoch 292 completed. Average Loss: 0.3008\n",
            "Epoch 293/10000, Batch 0/176, Loss: 0.2695\n",
            "Epoch 293/10000, Batch 100/176, Loss: 0.3288\n",
            "Epoch 293 completed. Average Loss: 0.3023\n",
            "Epoch 294/10000, Batch 0/176, Loss: 0.3005\n",
            "Epoch 294/10000, Batch 100/176, Loss: 0.2968\n",
            "Epoch 294 completed. Average Loss: 0.3014\n",
            "Epoch 295/10000, Batch 0/176, Loss: 0.2493\n",
            "Epoch 295/10000, Batch 100/176, Loss: 0.2910\n",
            "Epoch 295 completed. Average Loss: 0.2986\n",
            "Epoch 296/10000, Batch 0/176, Loss: 0.3056\n",
            "Validation Loss at iteration 52000: 15.4547\n",
            "Epoch 296/10000, Batch 100/176, Loss: 0.2951\n",
            "Epoch 296 completed. Average Loss: 0.2983\n",
            "Epoch 297/10000, Batch 0/176, Loss: 0.2906\n",
            "Epoch 297/10000, Batch 100/176, Loss: 0.3103\n",
            "Epoch 297 completed. Average Loss: 0.2952\n",
            "Epoch 298/10000, Batch 0/176, Loss: 0.2809\n",
            "Epoch 298/10000, Batch 100/176, Loss: 0.3123\n",
            "Epoch 298 completed. Average Loss: 0.2931\n",
            "Epoch 299/10000, Batch 0/176, Loss: 0.2620\n",
            "Epoch 299/10000, Batch 100/176, Loss: 0.3065\n",
            "Epoch 299 completed. Average Loss: 0.2942\n",
            "Epoch 300/10000, Batch 0/176, Loss: 0.2872\n",
            "Epoch 300/10000, Batch 100/176, Loss: 0.3006\n",
            "Epoch 300 completed. Average Loss: 0.2969\n",
            "Epoch 301/10000, Batch 0/176, Loss: 0.2767\n",
            "Epoch 301/10000, Batch 100/176, Loss: 0.3091\n",
            "Epoch 301 completed. Average Loss: 0.2933\n",
            "Epoch 302/10000, Batch 0/176, Loss: 0.2773\n",
            "Validation Loss at iteration 53000: 15.4493\n",
            "Epoch 302/10000, Batch 100/176, Loss: 0.2920\n",
            "Epoch 302 completed. Average Loss: 0.2938\n",
            "Epoch 303/10000, Batch 0/176, Loss: 0.2916\n",
            "Epoch 303/10000, Batch 100/176, Loss: 0.2835\n",
            "Epoch 303 completed. Average Loss: 0.2933\n",
            "Epoch 304/10000, Batch 0/176, Loss: 0.2666\n",
            "Epoch 304/10000, Batch 100/176, Loss: 0.3267\n",
            "Epoch 304 completed. Average Loss: 0.2915\n",
            "Epoch 305/10000, Batch 0/176, Loss: 0.2754\n",
            "Epoch 305/10000, Batch 100/176, Loss: 0.3040\n",
            "Epoch 305 completed. Average Loss: 0.2899\n",
            "Epoch 306/10000, Batch 0/176, Loss: 0.2730\n",
            "Epoch 306/10000, Batch 100/176, Loss: 0.2829\n",
            "Epoch 306 completed. Average Loss: 0.2888\n",
            "Epoch 307/10000, Batch 0/176, Loss: 0.2704\n",
            "Epoch 307/10000, Batch 100/176, Loss: 0.2957\n",
            "Validation Loss at iteration 54000: 15.5572\n",
            "Epoch 307 completed. Average Loss: 0.2908\n",
            "Epoch 308/10000, Batch 0/176, Loss: 0.2691\n",
            "Epoch 308/10000, Batch 100/176, Loss: 0.2998\n",
            "Epoch 308 completed. Average Loss: 0.2888\n",
            "Epoch 309/10000, Batch 0/176, Loss: 0.2583\n",
            "Epoch 309/10000, Batch 100/176, Loss: 0.2945\n",
            "Epoch 309 completed. Average Loss: 0.2865\n",
            "Epoch 310/10000, Batch 0/176, Loss: 0.2824\n",
            "Epoch 310/10000, Batch 100/176, Loss: 0.3007\n",
            "Epoch 310 completed. Average Loss: 0.2863\n",
            "Epoch 311/10000, Batch 0/176, Loss: 0.2530\n",
            "Epoch 311/10000, Batch 100/176, Loss: 0.2643\n",
            "Epoch 311 completed. Average Loss: 0.2868\n",
            "Epoch 312/10000, Batch 0/176, Loss: 0.2525\n",
            "Epoch 312/10000, Batch 100/176, Loss: 0.2825\n",
            "Epoch 312 completed. Average Loss: 0.2842\n",
            "Epoch 313/10000, Batch 0/176, Loss: 0.2438\n",
            "Validation Loss at iteration 55000: 15.6474\n",
            "Epoch 313/10000, Batch 100/176, Loss: 0.2733\n",
            "Epoch 313 completed. Average Loss: 0.2837\n",
            "Epoch 314/10000, Batch 0/176, Loss: 0.2518\n",
            "Epoch 314/10000, Batch 100/176, Loss: 0.2862\n",
            "Epoch 314 completed. Average Loss: 0.2824\n",
            "Epoch 315/10000, Batch 0/176, Loss: 0.2522\n",
            "Epoch 315/10000, Batch 100/176, Loss: 0.2743\n",
            "Epoch 315 completed. Average Loss: 0.2809\n",
            "Epoch 316/10000, Batch 0/176, Loss: 0.2746\n",
            "Epoch 316/10000, Batch 100/176, Loss: 0.3159\n",
            "Epoch 316 completed. Average Loss: 0.2819\n",
            "Epoch 317/10000, Batch 0/176, Loss: 0.2356\n",
            "Epoch 317/10000, Batch 100/176, Loss: 0.2842\n",
            "Epoch 317 completed. Average Loss: 0.2807\n",
            "Epoch 318/10000, Batch 0/176, Loss: 0.2580\n",
            "Epoch 318/10000, Batch 100/176, Loss: 0.2947\n",
            "Epoch 318 completed. Average Loss: 0.2801\n",
            "Epoch 319/10000, Batch 0/176, Loss: 0.2690\n",
            "Validation Loss at iteration 56000: 15.6669\n",
            "Epoch 319/10000, Batch 100/176, Loss: 0.3238\n",
            "Epoch 319 completed. Average Loss: 0.2812\n",
            "Epoch 320/10000, Batch 0/176, Loss: 0.2691\n",
            "Epoch 320/10000, Batch 100/176, Loss: 0.3016\n",
            "Epoch 320 completed. Average Loss: 0.2784\n",
            "Epoch 321/10000, Batch 0/176, Loss: 0.2554\n",
            "Epoch 321/10000, Batch 100/176, Loss: 0.2460\n",
            "Epoch 321 completed. Average Loss: 0.2764\n",
            "Epoch 322/10000, Batch 0/176, Loss: 0.2474\n",
            "Epoch 322/10000, Batch 100/176, Loss: 0.2855\n",
            "Epoch 322 completed. Average Loss: 0.2776\n",
            "Epoch 323/10000, Batch 0/176, Loss: 0.2582\n",
            "Epoch 323/10000, Batch 100/176, Loss: 0.2570\n",
            "Epoch 323 completed. Average Loss: 0.2776\n",
            "Epoch 324/10000, Batch 0/176, Loss: 0.2610\n",
            "Epoch 324/10000, Batch 100/176, Loss: 0.2492\n"
          ]
        }
      ]
    }
  ]
}